{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9835849c",
   "metadata": {},
   "source": [
    "#### The SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611b7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc1a320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.231822967529297\n",
      "22.311845779418945\n",
      "21.685230255126953\n",
      "21.18732261657715\n",
      "20.765697479248047\n",
      "20.395889282226562\n",
      "20.064184188842773\n",
      "19.761991500854492\n",
      "19.483503341674805\n",
      "19.224586486816406\n",
      "18.9821834564209\n",
      "18.75394058227539\n",
      "18.538013458251953\n",
      "18.332921981811523\n",
      "18.13745880126953\n",
      "17.950620651245117\n",
      "17.771562576293945\n",
      "17.599571228027344\n",
      "17.434032440185547\n",
      "17.274415969848633\n",
      "17.120250701904297\n",
      "16.971139907836914\n",
      "16.826719284057617\n",
      "16.686668395996094\n",
      "16.550697326660156\n",
      "16.418556213378906\n",
      "16.290010452270508\n",
      "16.164852142333984\n",
      "16.042888641357422\n",
      "15.923945426940918\n",
      "15.807865142822266\n",
      "15.694502830505371\n",
      "15.583721160888672\n",
      "15.4753999710083\n",
      "15.369423866271973\n",
      "15.265680313110352\n",
      "15.164079666137695\n",
      "15.064526557922363\n",
      "14.966931343078613\n",
      "14.871219635009766\n",
      "14.777314186096191\n",
      "14.68514633178711\n",
      "14.594646453857422\n",
      "14.505756378173828\n",
      "14.418414115905762\n",
      "14.332568168640137\n",
      "14.248164176940918\n",
      "14.165155410766602\n",
      "14.083488464355469\n",
      "14.00312614440918\n",
      "13.924025535583496\n",
      "13.84614372253418\n",
      "13.769445419311523\n",
      "13.69389533996582\n",
      "13.619455337524414\n",
      "13.546097755432129\n",
      "13.473786354064941\n",
      "13.402495384216309\n",
      "13.332195281982422\n",
      "13.262856483459473\n",
      "13.194457054138184\n",
      "13.126969337463379\n",
      "13.060368537902832\n",
      "12.994632720947266\n",
      "12.929740905761719\n",
      "12.865670204162598\n",
      "12.80240249633789\n",
      "12.73991584777832\n",
      "12.678193092346191\n",
      "12.617215156555176\n",
      "12.556964874267578\n",
      "12.497425079345703\n",
      "12.438584327697754\n",
      "12.38041877746582\n",
      "12.322917938232422\n",
      "12.26606559753418\n",
      "12.209851264953613\n",
      "12.154254913330078\n",
      "12.09926986694336\n",
      "12.044879913330078\n",
      "11.991074562072754\n",
      "11.93783950805664\n",
      "11.88516616821289\n",
      "11.833041191101074\n",
      "11.781454086303711\n",
      "11.730393409729004\n",
      "11.679851531982422\n",
      "11.629816055297852\n",
      "11.580279350280762\n",
      "11.531230926513672\n",
      "11.482662200927734\n",
      "11.434564590454102\n",
      "11.38692855834961\n",
      "11.339747428894043\n",
      "11.293010711669922\n",
      "11.246713638305664\n",
      "11.200845718383789\n",
      "11.155400276184082\n",
      "11.110370635986328\n",
      "11.065751075744629\n"
     ]
    }
   ],
   "source": [
    "def train_with_lr(lr=1):\n",
    "    weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "    opt = SGD([weights], lr=lr)\n",
    "    for t in range(100):\n",
    "        opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "        loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "        print(loss.cpu().item())\n",
    "        loss.backward() # Run backward pass, which computes gradients.\n",
    "        opt.step() # Run optimizer step.\n",
    "train_with_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c78bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.613693237304688\n",
      "15.112763404846191\n",
      "11.140484809875488\n",
      "8.716239929199219\n",
      "7.060154914855957\n",
      "5.853676795959473\n",
      "4.936800003051758\n",
      "4.2186360359191895\n",
      "3.6431238651275635\n",
      "3.173565626144409\n",
      "2.784832239151001\n",
      "2.4590954780578613\n",
      "2.183340549468994\n",
      "1.947838544845581\n",
      "1.745171308517456\n",
      "1.5695844888687134\n",
      "1.4165500402450562\n",
      "1.2824575901031494\n",
      "1.1643961668014526\n",
      "1.059995174407959\n",
      "0.9673063158988953\n",
      "0.8847154378890991\n",
      "0.8108751773834229\n",
      "0.7446537017822266\n",
      "0.6850941181182861\n",
      "0.6313827633857727\n",
      "0.5828243494033813\n",
      "0.5388219952583313\n",
      "0.49886059761047363\n",
      "0.46249425411224365\n",
      "0.4293350875377655\n",
      "0.3990447521209717\n",
      "0.37132683396339417\n",
      "0.3459210693836212\n",
      "0.32259804010391235\n",
      "0.30115506052970886\n",
      "0.2814127206802368\n",
      "0.26321133971214294\n",
      "0.24640899896621704\n",
      "0.23087894916534424\n",
      "0.21650773286819458\n",
      "0.2031938135623932\n",
      "0.19084596633911133\n",
      "0.17938199639320374\n",
      "0.16872793436050415\n",
      "0.15881691873073578\n",
      "0.14958851039409637\n",
      "0.14098793268203735\n",
      "0.13296547532081604\n",
      "0.12547598779201508\n",
      "0.11847838759422302\n",
      "0.11193519830703735\n",
      "0.10581224411725998\n",
      "0.10007833689451218\n",
      "0.09470488876104355\n",
      "0.08966576308012009\n",
      "0.0849369689822197\n",
      "0.08049651235342026\n",
      "0.07632413506507874\n",
      "0.072401262819767\n",
      "0.06871074438095093\n",
      "0.06523679941892624\n",
      "0.06196485459804535\n",
      "0.05888145789504051\n",
      "0.055974189192056656\n",
      "0.053231533616781235\n",
      "0.05064285919070244\n",
      "0.048198290169239044\n",
      "0.04588867723941803\n",
      "0.0437055379152298\n",
      "0.0416409969329834\n",
      "0.03968770429491997\n",
      "0.037838853895664215\n",
      "0.03608810901641846\n",
      "0.034429553896188736\n",
      "0.03285767883062363\n",
      "0.031367357820272446\n",
      "0.029953796416521072\n",
      "0.028612518683075905\n",
      "0.02733934484422207\n",
      "0.02613036148250103\n",
      "0.024981914088129997\n",
      "0.02389058656990528\n",
      "0.022853167727589607\n",
      "0.021866653114557266\n",
      "0.020928235724568367\n",
      "0.020035268738865852\n",
      "0.019185278564691544\n",
      "0.018375935032963753\n",
      "0.01760505698621273\n",
      "0.016870586201548576\n",
      "0.016170594841241837\n",
      "0.015503264963626862\n",
      "0.014866888523101807\n",
      "0.014259852468967438\n",
      "0.013680645264685154\n",
      "0.013127836398780346\n",
      "0.01260007731616497\n",
      "0.012096099555492401\n",
      "0.011614706367254257\n"
     ]
    }
   ],
   "source": [
    "train_with_lr(1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fdcb78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.831697463989258\n",
      "21.831697463989258\n",
      "3.7457268238067627\n",
      "0.0896436795592308\n",
      "1.4007038418120563e-16\n",
      "1.5611711606934078e-18\n",
      "5.2570132587009346e-20\n",
      "3.1316387522597062e-21\n",
      "2.686522232062097e-22\n",
      "2.9850247373517037e-23\n",
      "4.032438827449519e-24\n",
      "6.354760727088093e-25\n",
      "1.1351687472012565e-25\n",
      "2.2509475706860925e-26\n",
      "4.8771132872294616e-27\n",
      "1.1406159280483048e-27\n",
      "2.851539820120762e-28\n",
      "7.5609018406698e-29\n",
      "2.112615901037936e-29\n",
      "6.1870789105247994e-30\n",
      "1.89060323583994e-30\n",
      "6.004643315751757e-31\n",
      "1.975619501189715e-31\n",
      "6.714248583238634e-32\n",
      "2.3511288793459106e-32\n",
      "8.464065523175293e-33\n",
      "3.126469671721731e-33\n",
      "1.1828929857347599e-33\n",
      "4.576947048689891e-34\n",
      "1.808579218359819e-34\n",
      "7.289237025617899e-35\n",
      "2.993041649765045e-35\n",
      "1.250771907022993e-35\n",
      "5.3145495787203145e-36\n",
      "2.294039393955804e-36\n",
      "1.0051615674922361e-36\n",
      "4.467385242649392e-37\n",
      "2.0126116683740813e-37\n",
      "9.185106738652679e-38\n",
      "4.2439924274923344e-38\n",
      "1.98425516080582e-38\n",
      "9.38286610546929e-39\n",
      "4.4852410921492664e-39\n",
      "2.166501312843277e-39\n",
      "1.0570064381325311e-39\n",
      "5.206860755830296e-40\n",
      "2.5887868089629536e-40\n",
      "1.2986533518130242e-40\n",
      "6.570968758911932e-41\n",
      "3.352606575897125e-41\n",
      "1.7242977603516874e-41\n",
      "8.937481605463683e-42\n",
      "4.6677251846659657e-42\n",
      "2.4550749094970795e-42\n",
      "1.3004049748934302e-42\n",
      "6.9364273984078445e-43\n",
      "3.7274539151040134e-43\n",
      "2.0178697886277366e-43\n",
      "1.0930128021733573e-43\n",
      "6.025583396596713e-44\n",
      "3.2229864679470793e-44\n",
      "1.8216880036222622e-44\n",
      "9.80908925027372e-45\n",
      "5.605193857299268e-45\n",
      "2.802596928649634e-45\n",
      "1.401298464324817e-45\n",
      "1.401298464324817e-45\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "train_with_lr(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b78fe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.975845336914062\n",
      "10099.279296875\n",
      "1744304.5\n",
      "194035248.0\n",
      "15716853760.0\n",
      "991913443328.0\n",
      "50921597829120.0\n",
      "2190864830955520.0\n",
      "8.075059925509734e+16\n",
      "2.592991740801581e+18\n",
      "7.351361131589403e+19\n",
      "1.8601283687547887e+21\n",
      "4.238549728860823e+22\n",
      "8.763297269289533e+23\n",
      "1.6545960435473895e+25\n",
      "2.8688559006199424e+26\n",
      "4.590169440991908e+27\n",
      "6.8062972269119495e+28\n",
      "9.388694720528329e+29\n",
      "1.208888584699385e+31\n",
      "1.4574030165905482e+32\n",
      "1.6496205584273464e+33\n",
      "1.7574709159504212e+34\n",
      "1.766386139213741e+35\n",
      "1.6783671763136253e+36\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "train_with_lr(1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674aae88",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1de533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params\t\tgradients\tadamw\t\tactivation\tsum\n",
      "7.624\t\t7.624\t\t15.249\t\t0.000\t\t30.497\n",
      "7.624\t\t7.624\t\t15.249\t\t18.075\t\t48.572\n",
      "7.624\t\t7.624\t\t15.249\t\t36.150\t\t66.647\n",
      "7.624\t\t7.624\t\t15.249\t\t54.225\t\t84.722\n",
      "7.624\t\t7.624\t\t15.249\t\t72.300\t\t102.797\n",
      "7.624\t\t7.624\t\t15.249\t\t90.375\t\t120.872\n",
      "days: 6583.556412243875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from transformer import TransformerLM\n",
    "from perf import *\n",
    "\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "\n",
    "d_ff = 4 * d_model\n",
    "\n",
    "print('params\\t\\tgradients\\tadamw\\t\\tactivation\\tsum')\n",
    "for batch_size in range(0, 6):\n",
    "    memory = CalcMemory(batch_size, vocab_size, context_length, num_layers, d_model, num_heads)\n",
    "\n",
    "    print('\\t\\t'.join(f'{x:.3f}' for x in memory))\n",
    "\n",
    "a100 = 19.5 * 1000**4 * 0.5\n",
    "flops = CalcFlops(1024, vocab_size, context_length, num_layers, d_model, num_heads) * 400000\n",
    "print(f'days: {flops/a100/3600/24}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28eb73e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape               Output Shape              Param #                   Mult-Adds\n",
       "===========================================================================================================================================================\n",
       "TransformerLM (TransformerLM)                           [4, 1024]                 [4, 1024, 50257]          --                        --\n",
       "├─Embedding (token_embeddings)                          [4, 1024]                 [4, 1024, 1600]           80,411,200                321,644,800\n",
       "├─ModuleList (layers)                                   --                        --                        --                        --\n",
       "│    └─TransformerBlock (0)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (1)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (2)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (3)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (4)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (5)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (6)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (7)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (8)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (9)                             [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (10)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (11)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (12)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (13)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (14)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (15)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (16)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (17)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (18)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (19)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (20)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (21)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (22)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (23)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (24)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (25)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (26)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (27)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (28)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (29)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (30)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (31)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (32)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (33)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (34)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (35)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (36)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (37)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (38)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (39)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (40)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (41)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (42)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (43)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (44)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (45)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (46)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "│    └─TransformerBlock (47)                            [4, 1024, 1600]           [4, 1024, 1600]           40,963,200                163,852,800\n",
       "├─RMSNorm (ln_final)                                    [4, 1024, 1600]           [4, 1024, 1600]           1,600                     6,400\n",
       "├─Linear (lm_head)                                      [4, 1024, 1600]           [4, 1024, 50257]          80,411,200                321,644,800\n",
       "===========================================================================================================================================================\n",
       "Total params: 2,127,057,600\n",
       "Trainable params: 2,127,057,600\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.51\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 39500.41\n",
       "Params size (MB): 8508.23\n",
       "Estimated Total Size (MB): 48008.68\n",
       "==========================================================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerLM(vocab_size, context_length, num_layers, d_model, num_heads, d_ff, 10000, dtype=torch.float32)\n",
    "batch_size = 4\n",
    "summary(\n",
    "    model, \n",
    "    input_size=(batch_size, context_length),\n",
    "    dtypes=[torch.long], \n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params\t\tgradients\tadamw\t\tactivation\tsum\n",
      "0.079\t\t0.079\t\t0.159\t\t0.351\t\t0.668\n",
      "Using device: cuda\n",
      "[1] Model weights memory: 0.0862 GB\n",
      "[2] Post-forward memory usage (Weights + Activations): 0.7241 GB\n",
      "--------------------------------------------------\n",
      "Actual Training Peak Memory: 0.7578 GB\n",
      "--------------------------------------------------\n",
      "Detailed Breakdown:\n",
      "  - Model weights: 0.0862 GB\n",
      "  - Activations (est.): 0.6379 GB (intermediate variables from forward pass)\n",
      "  - Gradients & temp buffers: 0.0337 GB (additional overhead from backward pass)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from perf import *\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "context_length = 128\n",
    "num_layers = 48\n",
    "d_model = 160\n",
    "num_heads = 4\n",
    "d_ff = 4 * d_model\n",
    "batch_size = 4\n",
    "\n",
    "# Display theoretical estimation\n",
    "print('params\\t\\tgradients\\tadamw\\t\\tactivation\\tsum')\n",
    "memory = CalcMemory(batch_size, vocab_size, context_length, num_layers, d_model, num_heads)\n",
    "print('\\t\\t'.join(f'{x:.3f}' for x in memory))\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"GPU not detected. Memory measurement is only supported on CUDA devices.\")\n",
    "else:\n",
    "    # --- Step 1: Initialize Model and Measure Static Weights ---\n",
    "    # Clear cache and reset stats before starting\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Instantiate model and move to GPU\n",
    "    model = TransformerLM(\n",
    "        vocab_size, context_length, num_layers, d_model, num_heads, d_ff, \n",
    "        theta=10000, device=device, dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    weight_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"[1] Model weights memory: {weight_memory:.4f} GB\")\n",
    "\n",
    "    # --- Step 2: Measure Forward Pass (Activations) ---\n",
    "    # Prepare dummy data (Long type for Embedding layer)\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, context_length), device=device)\n",
    "\n",
    "    # Reset peak stats to capture the peak from this specific computation\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Forward pass: This allocates memory for activations (intermediate tensors)\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    forward_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"[2] Post-forward memory usage (Weights + Activations): {forward_memory:.4f} GB\")\n",
    "\n",
    "    # --- Step 3: Measure Backward Pass (Gradients & Peak) ---\n",
    "    # Backward pass: This calculates gradients, usually reaching the absolute peak usage\n",
    "    loss = outputs.mean()  # Create a dummy loss scalar\n",
    "    loss.backward()\n",
    "\n",
    "    # --- Step 4: Final Statistics ---\n",
    "    # max_memory_allocated() records the highest point seen since the last reset\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"Actual Training Peak Memory: {peak_memory:.4f} GB\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"Detailed Breakdown:\")\n",
    "    print(f\"  - Model weights: {weight_memory:.4f} GB\")\n",
    "    print(f\"  - Activations (est.): {forward_memory - weight_memory:.4f} GB (intermediate variables from forward pass)\")\n",
    "    print(f\"  - Gradients & temp buffers: {peak_memory - forward_memory:.4f} GB (additional overhead from backward pass)\")\n",
    "    print(f\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
